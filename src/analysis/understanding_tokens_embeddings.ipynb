{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding tokenization and embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES IMPORT\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI API\n",
    "client = OpenAI()\n",
    "# Define the embdedding model\n",
    "EMBEDDING_MODEL = \"text-embedding-3-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of converting text into smaller sub-texts, which are called tokens. In the context of NLP, tokens are words, phrases, or sentences. Tokenization is a crucial step in NLP because it helps to break down the text into smaller units, which can be used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step : Define the text to be tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : What is DNA ?\n"
     ]
    }
   ],
   "source": [
    "question_chars = \"What is DNA ?\"\n",
    "print(f\"question : {question_chars}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step : Tokenize the text using the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting the question into tokens : ['what', 'is', 'dna', '?']\n"
     ]
    }
   ],
   "source": [
    "t = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "question_tokens = t.tokenize(question_chars)\n",
    "print(f\"Splitting the question into tokens : {question_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens : 4\n",
      "Tokens id : [3923, 374, 15922, 949]\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "question_enc = encoding.encode(question_chars)\n",
    "print(f\"Number of tokens : {len(question_enc)}\")\n",
    "print(f\"Tokens id : {question_enc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 : Get the embeddings for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_raw(text: str, model: str = EMBEDDING_MODEL) -> str:\n",
    "    \"\"\"Get raw embeddings from OpenAi.\"\"\"\n",
    "    return client.embeddings.create(\n",
    "           input=[text],\n",
    "           model=model\n",
    "        ).data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token : what\n",
      "Token embedding shape : 3072\n",
      "Token embedding saved in what_emb.txt\n",
      "Token : is\n",
      "Token embedding shape : 3072\n",
      "Token embedding saved in is_emb.txt\n",
      "Token : dna\n",
      "Token embedding shape : 3072\n",
      "Token embedding saved in dna_emb.txt\n",
      "Token : ?\n",
      "Token embedding shape : 3072\n",
      "Token embedding saved in ?_emb.txt\n"
     ]
    }
   ],
   "source": [
    "for token in question_tokens:\n",
    "    print(f\"Token : {token}\")\n",
    "    token_emb = get_embeddings_raw(token)\n",
    "    print(f\"Token embedding shape : {len(token_emb)}\")\n",
    "    # save the embeddings in a text file\n",
    "    with open(f\"{token}_emb.txt\", \"w\") as f:\n",
    "        f.write(f\"{token_emb}\\n\")\n",
    "    print(f\"Token embedding saved in {token}_emb.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biopyassistant-sandbox-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
